{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import seaborn as sns\n",
    "from torch_geometric.utils import to_undirected\n",
    "from data_utils import eval_acc, eval_rocauc, load_fixed_splits\n",
    "from utils import load_dataset, edgeindex_construct\n",
    "from models import *\n",
    "import uuid\n",
    "import torch.optim as optim\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_undirected\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m eval_acc, eval_rocauc, load_fixed_splits\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, feature, label, index, eval_func, criterion):\n",
    "    model.eval()\n",
    "    out = model(feature[index])\n",
    "    acc = eval_func(label[index], out)\n",
    "    return acc\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='General Training Pipeline')\n",
    "    parser.add_argument('--seed',type=int, default=51290)\n",
    "    parser.add_argument('--dev',type=int, default=0)\n",
    "    parser.add_argument('--dataset', type=str, default='genius')\n",
    "\n",
    "    parser.add_argument('--K', type=int, default=10)\n",
    "    parser.add_argument('--hid', type=int, default=128)\n",
    "    parser.add_argument('--nlayers', type=int, default=3, help='number of layers for MLP')\n",
    "\n",
    "    parser.add_argument('--model', type=str, choices=['mlp', 'gfk'], default='gfk')\n",
    "    parser.add_argument('--epochs', type=int, default=1000)\n",
    "    parser.add_argument('--runs', type=int, default=5, help='number of distinct runs')\n",
    "    parser.add_argument('--dropout', type=float, default=0.5)\n",
    "    parser.add_argument('--weight_decay', type=float, default=1e-3)\n",
    "    parser.add_argument('--lr', type=float, default=0.01)\n",
    "    parser.add_argument('--dprate', type=float, default=0.5)\n",
    "\n",
    "    parser.add_argument('--plain', action=\"store_true\", help='if plain basis')\n",
    "\n",
    "    parser.add_argument('--patience', type=int, default=100, help='Number of epochs to train.')\n",
    "    parser.add_argument('--lr1', type=float, default=0.01, help='Initial learning rate of MLP.')\n",
    "    parser.add_argument('--lr2', type=float, default=0.01, help='Initial learning rate of Combination.')\n",
    "    parser.add_argument('--wd1', type=float, default=5e-4, help='Weight decay of MLP.')\n",
    "    parser.add_argument('--wd2', type=float, default=5e-4, help='Weight decay of Combination.')\n",
    "    parser.add_argument('--sole', action=\"store_true\", help='if one paramter for one level feature')\n",
    "    parser.add_argument('--dpC', type=float, default=0.5, help='Dropout rate of Combination.')\n",
    "    parser.add_argument('--dpM', type=float, default=0.5, help='Dropout rate of MLP.')\n",
    "    parser.add_argument('--tau', type=float, default=0.5, help='tau.')\n",
    "    parser.add_argument('--to_undirected', action=\"store_true\", help='if to_undirected')\n",
    "    parser.add_argument('--self_loop', action=\"store_true\", default=False, help='if self_loop')\n",
    "    parser.add_argument('--bias', default='none', help='bias.')\n",
    "    parser.add_argument('--log_dir', type=str,  default='./report')\n",
    "\n",
    "    # Optuna Settings\n",
    "    parser.add_argument('--optruns', type=int, default=100)\n",
    "    parser.add_argument('--path', type=str, default=\"\")\n",
    "    parser.add_argument('--name', type=str, default=\"opt\")\n",
    "\n",
    "    # Train settings\n",
    "    args = parser.parse_args()\n",
    "    print(args)\n",
    "    print(\"---------------------------------------------\")\n",
    "    return args\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def GraphConstruct(edge_index, n):\n",
    "    graph = []\n",
    "    for i in range(n):\n",
    "        edge = []\n",
    "        graph.append(edge)\n",
    "    m = edge_index.shape[1]\n",
    "    for i in range(m):\n",
    "        u,v=edge_index[0][i], edge_index[1][i]\n",
    "        graph[u].append(v)\n",
    "    return graph\n",
    "\n",
    "def homocal(graph, train_idx, labels):\n",
    "    n = labels.shape[0]\n",
    "    train = np.array([False]*n)\n",
    "    train[train_idx]=True\n",
    "    edge = 0.0\n",
    "    cnt = 0.0\n",
    "    for node in train_idx:\n",
    "        for nei in graph[node]:\n",
    "            if train[nei]:\n",
    "                edge += 1.0\n",
    "                if labels[node]==labels[nei]:\n",
    "                    cnt += 1.0\n",
    "    return cnt/edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_args()\n",
    "set_seed(args.seed)\n",
    "device = f'cuda:{args.dev}' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device)\n",
    "\n",
    "#load fixed dataset split\n",
    "split_idx_lst = load_fixed_splits(args.dataset)\n",
    "dataset_str = 'data/' + args.dataset +'/'+args.dataset+'.npz'\n",
    "data = np.load(dataset_str)\n",
    "edge_index, feat, labels=data['edge_index'], data['feats'], data['labels'] \n",
    "num_nodes = labels.shape[0]\n",
    "graph = GraphConstruct(edge_index, num_nodes) \n",
    "\n",
    "label = torch.LongTensor(labels)\n",
    "if len(label.shape) == 1:\n",
    "    label = label.unsqueeze(1)\n",
    "label = label.to(device)   \n",
    "\n",
    "\n",
    "LP, _,_ = edgeindex_construct(edge_index, num_nodes, args.self_loop)    \n",
    "feat=torch.FloatTensor(feat)\n",
    "\n",
    "if args.dataset == 'genius':\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    eval_func = eval_rocauc\n",
    "else:\n",
    "    criterion = nn.NLLLoss()\n",
    "    eval_func = eval_acc\n",
    "\n",
    "### Training loop ###\n",
    "results = []\n",
    "\n",
    "checkpt_file = 'pretrained/'+uuid.uuid4().hex+'.pt'\n",
    "\n",
    "for run in range(args.runs):\n",
    "    split_idx = split_idx_lst[run]\n",
    "    train_idx = split_idx['train']\n",
    "    homoratio = homocal(graph, train_idx, labels)\n",
    "    print(run, '-homoration: ', homoratio)     \n",
    "    features, dim = load_dataset(LP, feat, args.K, args.tau, homoratio, args.plain)\n",
    "    train_idx=train_idx.to(device)\n",
    "    # Model and optimizer\n",
    "    if args.model =='mlp':\n",
    "        model = MLP(nfeat=features.shape[1],\n",
    "            nlayers=args.nlayers,\n",
    "            nhidden=args.hid,\n",
    "            nclass=label.max().item() + 1,\n",
    "            dropout=args.dropout,\n",
    "            bias = args.bias).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "    elif args.model =='gfk':    \n",
    "        model = GFK(level=args.K,\n",
    "            nfeat=dim,\n",
    "            nlayers=args.nlayers,\n",
    "            nhidden=args.hid,\n",
    "            nclass=label.max().item() + 1,\n",
    "            dropoutC=args.dpC,\n",
    "            dropoutM=args.dpM,\n",
    "            bias = args.bias,\n",
    "            sole = args.sole).to(device)            \n",
    "\n",
    "        optimizer = optim.AdamW([{\n",
    "                'params': model.mlp.parameters(),\n",
    "                'weight_decay': args.wd1,\n",
    "                'lr': args.lr1\n",
    "            }, {\n",
    "                'params':model.comb.parameters(),\n",
    "                'weight_decay': args.wd2,\n",
    "                'lr': args.lr2\n",
    "            }])\n",
    "        features=features.view(-1, args.K+1, dim)\n",
    "    else:\n",
    "        raise ValueError('wrong model para')\n",
    "    \n",
    "    features = features.to(device)\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    best_val_acc = best_test_acc = 0\n",
    "    best_val_loss = float('inf')\n",
    "    val_loss_history = []\n",
    "    val_acc_history = []\n",
    "    bad_counter = 0\n",
    "    for epoch in range(args.epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(features[train_idx])\n",
    "\n",
    "        if args.dataset =='genius':\n",
    "            if label.shape[1] == 1:\n",
    "                true_label = F.one_hot(label, label.max() + 1).squeeze(1)\n",
    "            else:\n",
    "                true_label = label\n",
    "            loss = criterion(out, true_label.squeeze(1)[train_idx].to(torch.float))\n",
    "        else:\n",
    "            out = F.log_softmax(out, dim=1)\n",
    "            loss = criterion(out, label.squeeze(1)[train_idx]) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        val_acc = evaluate(model, features, label, split_idx['valid'], eval_func, criterion)\n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch: {epoch:02d}, 'f'Loss: {loss:.4f}, 'f'Valid: {100 * val_acc:.2f}%, ')\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), checkpt_file)\n",
    "    model.load_state_dict(torch.load(checkpt_file))\n",
    "    test_acc = evaluate(model, features, label, split_idx['test'], eval_func, criterion)\n",
    "    print(f'best_val_acc:{100*best_val_acc:.2f}%, test acc {100*test_acc:.2f}')\n",
    "    results.append([test_acc,best_val_acc])\n",
    "    os.remove(checkpt_file)\n",
    "test_acc_mean, val_acc_mean= np.mean(results, axis=0) * 100\n",
    "test_acc_std = np.sqrt(np.var(results, axis=0)[0]) * 100\n",
    "values=np.asarray(results)[:,0]\n",
    "uncertainty=np.max(np.abs(sns.utils.ci(sns.algorithms.bootstrap(values,func=np.mean,n_boot=1000),95)-values.mean()))\n",
    "print(f'Dataset {args.dataset}, in {args.runs} repeated experiment:')\n",
    "print(f'test acc mean = {test_acc_mean:.4f} Â± {uncertainty*100:.4f}  \\t val acc mean = {val_acc_mean:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
